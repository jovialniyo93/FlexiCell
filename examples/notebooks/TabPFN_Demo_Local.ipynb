{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tisgLOuqzrfo"
      },
      "source": [
        "# TabPFN Hands-On-Demo\n",
        "\n",
        "**Welcome to this enhanced and educational walkthrough of TabPFN!**\n",
        "\n",
        "TabPFN is a novel machine learning model that is exceptionally fast and requires no hyperparameter tuning. It is particularly powerful for tabular data, which is a common data format in many real-world applications. This notebook will not only demonstrate how to use TabPFN but also provide clear explanations of the underlying concepts.\n",
        "\n",
        "**What you will learn:**\n",
        "\n",
        "* What TabPFN is and why it's a game-changer for tabular data.\n",
        "* How to set up and run TabPFN on your local machine or using the client API.\n",
        "* How to use TabPFN for both classification and regression tasks.\n",
        "* A comparison of TabPFN's performance against other popular models like XGBoost and RandomForest.\n",
        "* Advanced features of TabPFN, including handling text data, unsupervised learning, and model interpretability.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1.  [Installation](#installation)\n",
        "2.  [Backend Selection](#backend-selection)\n",
        "3.  [**Classification with TabPFN**](#classification)\n",
        "4.  [**Regression with TabPFN**](#regression)\n",
        "5.  [Handling Text Data](#text-data)\n",
        "6.  [Unsupervised Learning with TabPFN](#unsupervised-learning)\n",
        "   - Data Imputation\n",
        "   - Anomaly Detection\n",
        "   - Clustering Applications\n",
        "7.  [Model Interpretability](#interpretability)\n",
        "   - Feature Importance Analysis\n",
        "   - Explaining Predictions with SHAP\n",
        "   - Extracting and Visualizing Embeddings\n",
        "8.  [Predictive Behavior](#behavior)\n",
        "9.  [**Time Series Prediction with TabPFN**](#timeseries)\n",
        "9.  [Using TabPFN to estimate the effect of causal interventions](#causal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o03aOVAw0Etg"
      },
      "source": [
        "# Installation [Running this cell required!] <a name=\"installation\"></a>\n",
        "\n",
        "First, we need to install the necessary libraries. This includes TabPFN itself, as well as other libraries for data manipulation and baseline model comparisons. If you are asked to restart the runtime after installation, please do so.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "agAt8X7T0N6P",
        "outputId": "8be5e774-7ebb-42d2-ada1-6aead4a7e1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 248ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 127ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 124ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 123ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 138ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 133ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "## Base library Installation\n",
        "# Install Baselines for model comparison\n",
        "!uv pip install catboost xgboost\n",
        "\n",
        "# Install the datasets library for loading example data\n",
        "!uv pip install datasets\n",
        "\n",
        "# Install rich for better and more readable printing\n",
        "!uv pip install rich\n",
        "\n",
        "## TabPFN Installation optimized for Google Colab\n",
        "# Install the TabPFN Client library\n",
        "!uv pip install tabpfn-client\n",
        "\n",
        "# Install TabPFN extensions for additional functionalities\n",
        "!uv pip install tabpfn-extensions[all]\n",
        "\n",
        "# Install tabpfn\n",
        "!uv pip install tabpfn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8RTKDLrJAAr"
      },
      "source": [
        "***Note: remember to restart the runtime after the installation.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iro4vB3w4rgE"
      },
      "source": [
        "# Necessary Imports for the Notebook [Running this cell required!]\n",
        "\n",
        "Now that the libraries are installed, let's import all the necessary modules for this notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uVEqqRwQ4W1H"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "\n",
        "# TabPFN and Extensions\n",
        "\n",
        "try:\n",
        "    from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import (\n",
        "        AutoTabPFNClassifier,\n",
        "    )\n",
        "\n",
        "    from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
        "except ImportError:\n",
        "    raise ImportError(\n",
        "        \"Warning: Could not import TabPFN / TabPFN extensions. Please run installation above and restart the session afterwards (Runtime > Restart Session).\"\n",
        "    )\n",
        "\n",
        "# Data Science & Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Other ML Models\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "\n",
        "# Notebook UI/Display\n",
        "from IPython.display import Markdown, display\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.prompt import Prompt\n",
        "from rich.rule import Rule\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "\n",
        "# Scikit-Learn: Data & Preprocessing\n",
        "from sklearn.datasets import fetch_openml, load_breast_cancer\n",
        "\n",
        "# Scikit-Learn: Models\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
        "from sklearn.model_selection import (\n",
        "    KFold,\n",
        "    StratifiedKFold,\n",
        "    cross_val_score,\n",
        "    train_test_split,\n",
        ")\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "# This transformer will be used to handle categorical features for the baseline models\n",
        "column_transformer = make_column_transformer(\n",
        "    (\n",
        "        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
        "        make_column_selector(dtype_include=[\"object\", \"category\"]),\n",
        "    ),\n",
        "    remainder=\"passthrough\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I4uUwflTY3i"
      },
      "source": [
        "# Backend Selection [Running this cell required!] <a name=\"backend-selection\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUq82Au7UdXk"
      },
      "source": [
        "\n",
        "TabPFN can be run in two ways: locally on your machine (if you have a GPU) or by using the TabPFN client, which sends the data to a server for computation.\n",
        "In your project you would either use the **local** version (which requires a GPU) with:\n",
        "```python\n",
        "# Simple import for TabPFN\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "# Now you can use it like any other sklearn classifier\n",
        "# model = TabPFNClassifier()\n",
        "print(\"TabPFNClassifier imported successfully.\")\n",
        "```\n",
        "\n",
        "or the **client** API (which uses a remote server):\n",
        "\n",
        "```python\n",
        "# Simple import for TabPFN\n",
        "from tabpfn_client import TabPFNClassifier\n",
        "\n",
        "# Now you can use it like any other sklearn classifier\n",
        "# model = TabPFNClassifier()\n",
        "print(\"TabPFNClassifier imported successfully.\")\n",
        "```\n",
        "\n",
        "If you select **local** and it's your first time using the model, you'll need to accept the license agreement and log into Hugging Face.\n",
        "You'll be prompted with instructions on how to do this the first time you fit the model.\n",
        "\n",
        "For demonstration purposes, the cell below provides an interactive way to switch between local mode and the client:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "pJVAXg2YoDdF",
        "outputId": "0bff23cf-3116-4614-d5fc-62de8d9a1035"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭────────────────────────────────╮\n",
              "│ \u001b[1;35mTabPFN Demo: Backend Selection\u001b[0m │\n",
              "╰────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────────────╮\n",
              "│ <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TabPFN Demo: Backend Selection</span> │\n",
              "╰────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "This script can run TabPFN using one of two backends:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "This script can run TabPFN using one of two backends:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  \u001b[1;36m1\u001b[0m\u001b[1m. local:\u001b[0m Uses a local GPU \u001b[1m(\u001b[0mNVIDIA\u001b[1m)\u001b[0m. Requires CUDA.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. local:</span> Uses a local GPU <span style=\"font-weight: bold\">(</span>NVIDIA<span style=\"font-weight: bold\">)</span>. Requires CUDA.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  \u001b[1;36m2\u001b[0m\u001b[1m. client:\u001b[0m Uses the TabPFN API. Requires an internet connection and a free account.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">. client:</span> Uses the TabPFN API. Requires an internet connection and a free account.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1mChoose your backend\u001b[0m - If not field to enter is shown restart the cell. \u001b[1;35m[client/local]\u001b[0m \u001b[1;36m(client)\u001b[0m: "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">Choose your backend</span> - If not field to enter is shown restart the cell. <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">[client/local]</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(client)</span>: </pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "console = Console()\n",
        "\n",
        "console.print(Panel.fit(\"[bold magenta]TabPFN Demo: Backend Selection[/bold magenta]\"))\n",
        "console.print(\"\\nThis script can run TabPFN using one of two backends:\")\n",
        "console.print(\"  [bold]1. local:[/bold] Uses a local GPU (NVIDIA). Requires CUDA.\")\n",
        "console.print(\n",
        "    \"  [bold]2. client:[/bold] Uses the TabPFN API. Requires an internet connection and a free account.\"\n",
        ")\n",
        "\n",
        "backend = Prompt.ask(\n",
        "    \"\\n[bold]Choose your backend[/bold] - If not field to enter is shown restart the cell.\",\n",
        "    choices=[\"client\", \"local\"],\n",
        "    default=\"client\",\n",
        ")\n",
        "\n",
        "console.print(\n",
        "    f\"\\n✅ You have selected the '[bold green]{backend}[/bold green]' backend.\"\n",
        ")\n",
        "\n",
        "console.print(Rule(f\"[bold]Setting up [cyan]{backend}[/cyan] backend[/bold]\"))\n",
        "\n",
        "if backend == \"local\":\n",
        "    !uv pip install huggingface_hub\n",
        "\n",
        "    console.print(\"Attempting local backend setup...\")\n",
        "    import torch\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        console.print(\n",
        "            \"[bold red]Error:[/bold red] GPU device not found. For fast training, please enable GPU.\",\n",
        "            style=\"red\",\n",
        "        )\n",
        "        console.print(\n",
        "            \"In Colab: Go to [bold]Runtime -> Change runtime type -> Hardware accelerator -> GPU.[/bold]\",\n",
        "            style=\"yellow\",\n",
        "        )\n",
        "        raise SystemError(\"GPU device not found.\")\n",
        "    console.print(\n",
        "        \"[bold green]✅ GPU is available.[/bold green] Importing local TabPFN library...\"\n",
        "    )\n",
        "\n",
        "    from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
        "\n",
        "    console.print(\"[bold green]✅ TabPFN (local) imported successfully.[/bold green]\")\n",
        "elif backend == \"client\":\n",
        "    console.print(\"Attempting client backend setup...\")\n",
        "    console.print(\"Importing TabPFN client library...\")\n",
        "    from tabpfn_client import TabPFNClassifier, TabPFNRegressor, init\n",
        "\n",
        "    init()\n",
        "    console.print(\"[bold green]✅ TabPFN (client) initialized.[/bold green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8iQ9KEd1h9R"
      },
      "source": [
        "# Classification with TabPFN <a name=\"classification\"></a>\n",
        "\n",
        "Now, let's dive into a practical example of using TabPFN for a classification task. We will use the well-known Parkinson's Disease dataset. The goal is to predict the presence of Parkinson's disease based on various voice measurements.\n",
        "\n",
        "We will compare TabPFN's performance against other popular machine learning models: RandomForest, XGBoost, and CatBoost. The performance metric we will use is the [ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Parkinson’s Disease Dataset**\n",
        "\n",
        "**Source:**  \n",
        "The dataset was created by _Max Little_ of the **University of Oxford**, in collaboration with the **National Centre for Voice and Speech**, Denver, Colorado. The dataset consists of biomedical voice measurements collected from individuals, and the original study published the feature extraction methods for general voice disorders.\n",
        "\n",
        "### **Dataset Overview**\n",
        "\n",
        "-   **Number of subjects:** 31\n",
        "    \n",
        "-   **Number of Parkinson’s disease (PD) patients:** 23\n",
        "    \n",
        "-   **Total recordings:** 195 voice samples\n",
        "    \n",
        "-   **Goal:** To discriminate healthy individuals (status = 0) from those with Parkinson’s disease (status = 1).\n",
        "    \n",
        "-   **Identifier column:** `name` (subject name and recording number)\n",
        "    \n",
        "\n",
        "<details>\n",
        "If you use this dataset, please cite:\n",
        "\n",
        "> Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008).  \n",
        "> _Suitability of dysphonia measurements for telemonitoring of Parkinson's disease._  \n",
        "> IEEE Transactions on Biomedical Engineering.\n",
        "\n",
        "* * *\n",
        "\n",
        "\n",
        "\n",
        "* * *\n",
        "\n",
        "### **Attribute Information**\n",
        "\n",
        "Each row corresponds to a voice recording, and each column represents a distinct biomedical voice measure.\n",
        "\n",
        "**Feature Name**\n",
        "\n",
        "**Description**\n",
        "\n",
        "`name`\n",
        "\n",
        "ASCII subject name and recording number\n",
        "\n",
        "`MDVP:Fo(Hz)`\n",
        "\n",
        "Average vocal fundamental frequency\n",
        "\n",
        "`MDVP:Fhi(Hz)`\n",
        "\n",
        "Maximum vocal fundamental frequency\n",
        "\n",
        "`MDVP:Flo(Hz)`\n",
        "\n",
        "Minimum vocal fundamental frequency\n",
        "\n",
        "`MDVP:Jitter(%)`, `MDVP:Jitter(Abs)`, `MDVP:RAP`, `MDVP:PPQ`, `Jitter:DDP`\n",
        "\n",
        "Measures of variation in fundamental frequency\n",
        "\n",
        "`MDVP:Shimmer`, `MDVP:Shimmer(dB)`, `Shimmer:APQ3`, `Shimmer:APQ5`, `MDVP:APQ`, `Shimmer:DDA`\n",
        "\n",
        "Measures of variation in amplitude\n",
        "\n",
        "`NHR`, `HNR`\n",
        "\n",
        "Ratios of noise to tonal components in the voice\n",
        "\n",
        "`status`\n",
        "\n",
        "Health status of the subject (`0` = healthy, `1` = Parkinson’s disease)\n",
        "\n",
        "`RPDE`, `D2`\n",
        "\n",
        "Nonlinear dynamical complexity measures\n",
        "\n",
        "`DFA`\n",
        "\n",
        "Signal fractal scaling exponent\n",
        "\n",
        "`spread1`, `spread2`, `PPE`\n",
        "\n",
        "Nonlinear measures of fundamental frequency variation\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "PNtRCHyLG1D5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTQe5IprKLV5"
      },
      "outputs": [],
      "source": [
        "# Load Parkinsons dataset described above\n",
        "\n",
        "import pandas as pd, io, zipfile, requests\n",
        "\n",
        "url_zip = \"https://archive.ics.uci.edu/static/public/174/parkinsons.zip\"\n",
        "with requests.get(url_zip) as r:\n",
        "    r.raise_for_status()\n",
        "    zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    df = pd.read_csv(zf.open(\"parkinsons.data\"))\n",
        "X, y = df.drop([\"status\", \"name\"], axis=1), df[\"status\"]\n",
        "\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAXhoOc2U2vJ"
      },
      "outputs": [],
      "source": [
        "# Alternative datasets (commented for reference):\n",
        "\n",
        "# Cholesterol dataset: Predict cholesterol levels\n",
        "# Features: Patient characteristics, medical measurements\n",
        "# Samples: 303 patients\n",
        "# Target: Cholesterol levels in mg/dl\n",
        "# df = fetch_openml('cholesterol', version=2, as_frame=True)\n",
        "\n",
        "# Heart Disease dataset (Statlog): Predict presence of heart disease\n",
        "# Features: Clinical and test measurements\n",
        "# Samples: 270 patients\n",
        "# Target: Binary heart disease diagnosis\n",
        "# df = fetch_openml(\"heart-statlog\", version=1)\n",
        "\n",
        "# Diabetes dataset: Predict diabetes presence\n",
        "# Features: Medical measurements, patient history\n",
        "# Samples: 768 patients\n",
        "# Target: Binary diabetes diagnosis\n",
        "# df = fetch_openml(\"diabetes\", version=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTHzDedu3DKR"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate the TabPFN classifier\n",
        "tabpfn_classifier = TabPFNClassifier(random_state=42)\n",
        "tabpfn_classifier.fit(X_train, y_train)\n",
        "y_pred_proba = tabpfn_classifier.predict_proba(X_test)\n",
        "\n",
        "# Calculate the ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "print(f\"TabPFN ROC AUC Score: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVs_HfxbVCs3"
      },
      "source": [
        "### Comparing TabPFN with Other Classifiers\n",
        "\n",
        "To get a better sense of TabPFN's performance, let's compare it with other popular classification models using cross-validation. This will give us a more robust estimate of each model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITuXXhqE7Fps"
      },
      "outputs": [],
      "source": [
        "# Compare different machine learning models by training each one multiple times\n",
        "# on different parts of the data and averaging their performance scores for a\n",
        "# more reliable performance estimate\n",
        "\n",
        "# Encode target labels to classes for baselines\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Define models\n",
        "models = [\n",
        "    (\"TabPFN\", TabPFNClassifier(random_state=42)),\n",
        "    (\n",
        "        \"RandomForest\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            RandomForestClassifier(random_state=42),\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"XGBoost\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            XGBClassifier(random_state=42),\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"CatBoost\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            CatBoostClassifier(random_state=42, verbose=0),\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Calculate scores\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
        "scoring = \"roc_auc_ovr\" if len(np.unique(y)) > 2 else \"roc_auc\"\n",
        "scores = {\n",
        "    name: cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=1, verbose=1)\n",
        "    for name, model in models\n",
        "}\n",
        "\n",
        "# Plot results\n",
        "df = pd.DataFrame(\n",
        "    [(k, v.mean()) for (k, v) in scores.items()], columns=[\"Model\", \"ROC AUC\"]\n",
        ")\n",
        "ax = df.plot(x=\"Model\", y=\"ROC AUC\", kind=\"bar\", figsize=(10, 6))\n",
        "ax.set_ylim(df[\"ROC AUC\"].min() * 0.995, min(1.0, df[\"ROC AUC\"].max() * 1.005))\n",
        "ax.set_title(f\"Model Comparison - {n_splits}-fold Cross-validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipG2Cpb7Jrc7"
      },
      "source": [
        "# Regression with TabPFN <a name=\"regression\"></a>\n",
        "\n",
        "Next, we'll explore how to use TabPFN for regression tasks. We will use the Boston Housing dataset, where the goal is to predict the median value of owner-occupied homes.\n",
        "\n",
        "We will measure performance using the [Root Mean Squared Error](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.root_mean_squared_error.html), and again, we will compare TabPFN with other popular regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Boston Housing Dataset**\n",
        "\n",
        "### **Dataset Overview**\n",
        "\n",
        "-   **Objective:** Predict the **median value of owner-occupied homes** in the Boston area based on various socioeconomic and environmental attributes.\n",
        "    \n",
        "-   **Number of Instances:** 506\n",
        "    \n",
        "-   **Number of Attributes:** 14 (13 numeric features + 1 target variable)\n",
        "    \n",
        "-   **Target Variable:** `MEDV` — Median value of owner-occupied homes (in $1000s)\n",
        "    \n",
        "-   **Class Type:** Numeric\n",
        "    \n",
        "-   **Class Index:** Last\n",
        "    \n",
        "\n",
        "* * *\n",
        "\n",
        "<details>\n",
        "\n",
        "\n",
        "**Citation:**\n",
        "\n",
        "> Harrison, D. and Rubinfeld, D.L. (1978).  \n",
        "> _Hedonic prices and the demand for clean air._  \n",
        "> _Journal of Environmental Economics & Management_, 5, 81–102.  \n",
        "> Used in Belsley, Kuh & Welsch, _Regression Diagnostics: Identifying Influential Data and Sources of Collinearity_, Wiley, 1980.\n",
        "\n",
        "\n",
        "### **Attribute Information**\n",
        "\n",
        "**Feature**\n",
        "\n",
        "**Description**\n",
        "\n",
        "`CRIM`\n",
        "\n",
        "Per capita crime rate by town\n",
        "\n",
        "`ZN`\n",
        "\n",
        "Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "\n",
        "`INDUS`\n",
        "\n",
        "Proportion of non-retail business acres per town\n",
        "\n",
        "`CHAS`\n",
        "\n",
        "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "\n",
        "`NOX`\n",
        "\n",
        "Nitric oxides concentration (parts per 10 million)\n",
        "\n",
        "`RM`\n",
        "\n",
        "Average number of rooms per dwelling\n",
        "\n",
        "`AGE`\n",
        "\n",
        "Proportion of owner-occupied units built prior to 1940\n",
        "\n",
        "`DIS`\n",
        "\n",
        "Weighted distances to five Boston employment centres\n",
        "\n",
        "`RAD`\n",
        "\n",
        "Index of accessibility to radial highways\n",
        "\n",
        "`TAX`\n",
        "\n",
        "Full-value property-tax rate per $10,000\n",
        "\n",
        "`PTRATIO`\n",
        "\n",
        "Pupil–teacher ratio by town\n",
        "\n",
        "`B`\n",
        "\n",
        "1000(Bk − 0.63)², where Bk is the proportion of Blacks by town\n",
        "\n",
        "`LSTAT`\n",
        "\n",
        "Percentage of lower status population\n",
        "\n",
        "`MEDV`\n",
        "\n",
        "Median value of owner-occupied homes (in $1000s)\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "Qzrj9m0fI32f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Boston Housing dataset\n",
        "\n",
        "import pandas as pd, requests\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
        "df_boston = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
        "                        delim_whitespace=True, header=None, names=cols)\n",
        "X, y = df_boston.drop(columns=[\"MEDV\"]), df_boston[\"MEDV\"]\n",
        "\n",
        "display(X.head())\n"
      ],
      "metadata": {
        "id": "4vbEBAKKIRaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaA7WBuEVdQy"
      },
      "outputs": [],
      "source": [
        "# Alternative datasets (commented for reference):\n",
        "\n",
        "# Liver Disorders dataset\n",
        "# Classification task: Predict liver disorder presence\n",
        "# Features: Blood test results, alcohol consumption\n",
        "# Samples: 345 patients\n",
        "# Target: Binary liver disorder diagnosis\n",
        "# df = fetch_openml(\"liver-disorders\", version=1)\n",
        "\n",
        "# Grid Stability dataset\n",
        "# Regression task: Predict electrical grid stability\n",
        "# Features: Power system measurements, grid parameters\n",
        "# Samples: 10,000 simulations\n",
        "# Target: Grid stability score\n",
        "# df = fetch_openml(data_id=44973, as_frame=True)\n",
        "\n",
        "# Concrete Compressive Strength dataset\n",
        "# Regression task: Predict concrete strength\n",
        "# Features: Concrete components (cement, water, aggregates)\n",
        "# Samples: 1,030 concrete samples\n",
        "# Target: Compressive strength in MPa\n",
        "# df = fetch_openml(data_id=44959, as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWqC16PL7R_T"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate the TabPFN regressor\n",
        "tabpfn_regressor = TabPFNRegressor(random_state=42)\n",
        "tabpfn_regressor.fit(X_train, y_train)\n",
        "y_pred = tabpfn_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Root Mean Squared Error\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"TabPFN RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itu5-vVRWEMA"
      },
      "source": [
        "### Comparing TabPFN with Other Regressors\n",
        "\n",
        "Now, let's see how TabPFN's regression performance stacks up against other models using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XLxgbqSPe55"
      },
      "outputs": [],
      "source": [
        "# Compare different machine learning models by training each one multiple times\n",
        "# on different parts of the data and averaging their performance scores for a\n",
        "# more reliable performance estimate\n",
        "\n",
        "# Define models\n",
        "models = [\n",
        "    (\"TabPFN\", TabPFNRegressor(random_state=42)),\n",
        "    (\n",
        "        \"RandomForest\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            RandomForestRegressor(random_state=42),\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"XGBoost\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            XGBRegressor(random_state=42),\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"CatBoost\",\n",
        "        make_pipeline(\n",
        "            column_transformer,  # string data needs to be encoded for model\n",
        "            CatBoostRegressor(random_state=42, verbose=0),\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Calculate scores\n",
        "scoring = \"r2\"\n",
        "n_splits = 3\n",
        "cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
        "scores = {\n",
        "    name: cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=1, verbose=1)\n",
        "    for name, model in models\n",
        "}\n",
        "\n",
        "\n",
        "# Plot results\n",
        "df = pd.DataFrame([(k, v.mean()) for (k, v) in scores.items()], columns=[\"Model\", \"R2\"])\n",
        "ax = df.plot(x=\"Model\", y=\"R2\", kind=\"bar\", figsize=(10, 6))\n",
        "ax.set_ylim(df[\"R2\"].min() * 0.99, df[\"R2\"].max() * 1.01)\n",
        "ax.set_title(\n",
        "    f\"Model Comparison - {n_splits}-fold Cross-validation \\n (Variance Explained - Larger is better)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGfjIdjsCg-"
      },
      "source": [
        "# Handling Text Data <a name=\"text-data\"></a>\n",
        "\n",
        "A powerful feature of the TabPFN server is its ability to handle text data directly, without the need for manual feature engineering. This simplifies the process of working with datasets that contain a mix of numerical and textual features.\n",
        "\n",
        "**Note:** This feature is only available when using the 'client' backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYBdSZnor-at"
      },
      "outputs": [],
      "source": [
        "if backend != \"client\":\n",
        "    console.print(\n",
        "        Panel(\n",
        "            \"[bold yellow]Text data can only be processed through the TabPFN server. Please restart the notebook and select the 'client' backend to run this section.[/bold yellow]\",\n",
        "            title=\"[bold yellow]Warning\",\n",
        "            border_style=\"yellow\",\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    # We will use gdown to download the dataset\n",
        "    !gdown --fuzzy \"17bJekFGIAxbrdcBeBIvcyZjneweY581E\"\n",
        "\n",
        "    # Load the clothing review dataset\n",
        "    # We restrict to 500 rows to make the example faster\n",
        "    df_text = pd.read_csv(\"cloth.csv\", index_col=0).dropna()[:500]\n",
        "\n",
        "    # Define features and target\n",
        "    y_text = df_text[\"Rating\"]\n",
        "    X_text = df_text.drop(columns=[\"Rating\"])\n",
        "\n",
        "    print(\"Text dataset loaded successfully!\")\n",
        "    display(X_text.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQkFQD_rWi1v"
      },
      "source": [
        "### Comparing Text Handling\n",
        "\n",
        "Now, let's compare how TabPFN handles text natively versus how a baseline model like RandomForest needs a specific text processing pipeline.\n",
        "\n",
        "For baselines, we will create a pipeline that converts strings to ordinal features.\n",
        "\n",
        "For TabPFN, we simply pass the raw data with the text column directly to the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktXNMVFCsEHZ"
      },
      "outputs": [],
      "source": [
        "if backend == \"client\":\n",
        "    # Encode target labels to classes for baselines\n",
        "    le = LabelEncoder()\n",
        "    y_text = le.fit_transform(y_text)\n",
        "\n",
        "    # Define models\n",
        "    models = [\n",
        "        (\"TabPFN-Text\", TabPFNClassifier(random_state=42)),\n",
        "        (\n",
        "            \"TabPFN\",\n",
        "            make_pipeline(\n",
        "                column_transformer,  # string data needs to be encoded for model\n",
        "                TabPFNClassifier(random_state=42),\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"RandomForest\",\n",
        "            make_pipeline(\n",
        "                column_transformer,  # string data needs to be encoded for model\n",
        "                RandomForestClassifier(random_state=42),\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"XGBoost\",\n",
        "            make_pipeline(\n",
        "                column_transformer,  # string data needs to be encoded for model\n",
        "                XGBClassifier(random_state=42),\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"CatBoost\",\n",
        "            make_pipeline(\n",
        "                column_transformer,  # string data needs to be encoded for model\n",
        "                CatBoostClassifier(random_state=42, verbose=0),\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # Calculate scores\n",
        "    cv = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
        "    scoring = \"roc_auc_ovr\" if len(np.unique(y_text)) > 2 else \"roc_auc\"\n",
        "    scores = {\n",
        "        name: cross_val_score(\n",
        "            model, X_text, y_text, cv=cv, scoring=scoring, n_jobs=1, verbose=1\n",
        "        ).mean()\n",
        "        for name, model in models\n",
        "    }\n",
        "\n",
        "    # Plot results\n",
        "    df = pd.DataFrame(list(scores.items()), columns=[\"Model\", \"ROC AUC\"])\n",
        "    ax = df.plot(x=\"Model\", y=\"ROC AUC\", kind=\"bar\", figsize=(10, 6))\n",
        "    ax.set_ylim(df[\"ROC AUC\"].min() * 0.995, min(1.0, df[\"ROC AUC\"].max() * 1.005))\n",
        "    ax.set_title(\"Model Comparison - 5-fold Cross-validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTnMbG9vXHkL"
      },
      "source": [
        "# Unsupervised Learning with TabPFN <a name=\"unsupervised-learning\"></a>\n",
        "\n",
        "TabPFN can also be used for unsupervised learning tasks like outlier detection and synthetic data generation. These features are available through the `tabpfn-extensions` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZddJXFa4zE"
      },
      "source": [
        "### Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npJnFdtTa_nS"
      },
      "outputs": [],
      "source": [
        "from tabpfn_extensions import unsupervised\n",
        "from tabpfn_extensions.unsupervised import experiments\n",
        "\n",
        "# Load and prepare breast cancer dataset\n",
        "df = load_breast_cancer(return_X_y=False)\n",
        "X, y = df[\"data\"], df[\"target\"]\n",
        "feature_names = df[\"feature_names\"]\n",
        "\n",
        "# Initialize TabPFN models\n",
        "model_unsupervised = unsupervised.TabPFNUnsupervisedModel(\n",
        "    tabpfn_clf=TabPFNClassifier(), tabpfn_reg=TabPFNRegressor()\n",
        ")\n",
        "\n",
        "# Select features for synthetic data generation\n",
        "# Example features: [mean texture, mean area, mean concavity]\n",
        "feature_indices = [4, 6, 12]\n",
        "\n",
        "# Run synthetic data generation experiment\n",
        "experiment = unsupervised.experiments.GenerateSyntheticDataExperiment(\n",
        "    task_type=\"unsupervised\"\n",
        ")\n",
        "\n",
        "results = experiment.run(\n",
        "    tabpfn=model_unsupervised,\n",
        "    X=torch.tensor(X),\n",
        "    y=torch.tensor(y),\n",
        "    attribute_names=feature_names,\n",
        "    temp=1.0,  # Temperature parameter for sampling\n",
        "    n_samples=X.shape[0] * 2,  # Generate twice as many samples as original data\n",
        "    indices=feature_indices,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aoNg2_K4k6E"
      },
      "source": [
        "### Outlier detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezircAHiZSKG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from tabpfn_extensions import unsupervised\n",
        "from tabpfn_extensions.unsupervised import experiments\n",
        "\n",
        "# Load data\n",
        "df = load_breast_cancer(return_X_y=False)\n",
        "X, y = df[\"data\"], df[\"target\"]\n",
        "attribute_names = df[\"feature_names\"]\n",
        "\n",
        "# Initialize models\n",
        "clf = TabPFNClassifier(n_estimators=4)\n",
        "reg = TabPFNRegressor(n_estimators=4)\n",
        "model_unsupervised = unsupervised.TabPFNUnsupervisedModel(\n",
        "    tabpfn_clf=clf, tabpfn_reg=reg\n",
        ")\n",
        "\n",
        "# Run outlier detection\n",
        "exp_outlier = unsupervised.experiments.OutlierDetectionUnsupervisedExperiment(\n",
        "    task_type=\"unsupervised\"\n",
        ")\n",
        "results = exp_outlier.run(\n",
        "    tabpfn=model_unsupervised,\n",
        "    X=torch.tensor(X, dtype=torch.float32),\n",
        "    y=torch.tensor(y),\n",
        "    attribute_names=attribute_names,\n",
        "    indices=[4, 12],  # Analyze features 4 and 12\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk6OxtxmJsr9"
      },
      "source": [
        "### Missing value imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hgAlgghJuF3"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load and Prepare Data ---\n",
        "# Load the breast cancer dataset\n",
        "df = load_breast_cancer(return_X_y=False)\n",
        "X, y = df[\"data\"], df[\"target\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test = train_test_split(\n",
        "    X,\n",
        "    test_size=0.33,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# --- 2. Introduce Missing Values ---\n",
        "# Create a copy of the test set to introduce missing values (NaNs)\n",
        "X_test_missing = X_test.copy()\n",
        "n_samples, n_features = X_test_missing.shape\n",
        "\n",
        "# Introduce missing values in the first three columns for demonstration\n",
        "missing_fraction = 0.3\n",
        "n_missing = int(n_samples * missing_fraction)\n",
        "\n",
        "for col_idx in range(3):\n",
        "    # Choose random rows to set to NaN\n",
        "    missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n",
        "    X_test_missing[missing_indices, col_idx] = np.nan\n",
        "\n",
        "print(f\"Introduced {np.isnan(X_test_missing).sum()} missing values into the test set.\")\n",
        "print(pd.DataFrame(X_test_missing[:, 0:3]).head())\n",
        "\n",
        "# --- 3. Initialize the Unsupervised Model ---\n",
        "# Initialize TabPFN models for regression and classification tasks.\n",
        "# The unsupervised model uses these to model the data distribution.\n",
        "clf = TabPFNClassifier(n_estimators=3)\n",
        "reg = TabPFNRegressor(n_estimators=3)\n",
        "\n",
        "# Initialize the main unsupervised model\n",
        "model_unsupervised = unsupervised.TabPFNUnsupervisedModel(\n",
        "    tabpfn_clf=clf,\n",
        "    tabpfn_reg=reg,\n",
        ")\n",
        "\n",
        "# --- 4. Fit and Impute ---\n",
        "# Fit the model on the complete training data (without missing values)\n",
        "print(\"Fitting the unsupervised model on the training data...\")\n",
        "model_unsupervised.fit(X_train)\n",
        "\n",
        "# Perform imputation on the test set\n",
        "print(\"Imputing missing values...\\n\")\n",
        "X_imputed_tensor = model_unsupervised.impute(\n",
        "    X_test_missing,\n",
        "    n_permutations=5,  # Fewer permutations for a quicker example\n",
        ")\n",
        "\n",
        "print(\"\\n------------------------\")\n",
        "\n",
        "# --- 5. Verify Results ---\n",
        "# Check that the imputed data no longer contains any NaN values\n",
        "n_missing_after = torch.isnan(X_imputed_tensor).sum().item()\n",
        "\n",
        "print(f\"\\nNumber of missing values after imputation: {n_missing_after}\")\n",
        "print(pd.DataFrame(X_imputed_tensor[:, 0:3]).head())\n",
        "\n",
        "# Optional: Calculate the Mean Squared Error for the imputed values,\n",
        "# since we know the original ground truth values.\n",
        "original_nan_mask = np.isnan(X_test_missing)\n",
        "imputed_values = X_imputed_tensor.numpy()[original_nan_mask]\n",
        "original_values = X_test[original_nan_mask]\n",
        "\n",
        "mse = np.mean((imputed_values - original_values) ** 2)\n",
        "print(f\"Mean Squared Error of imputed values vs. original values: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqZlQ09WfCe9"
      },
      "source": [
        "# Model Interpretability <a name=\"interpretability\"></a>\n",
        "\n",
        "Understanding *why* a model makes certain predictions is crucial for building trust and for debugging. The `tabpfn-extensions` library provides tools for model interpretability. We'll look at SHAP (SHapley Additive exPlanations) values, which show the impact of each feature on a specific prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RYILEoPXaMZ"
      },
      "source": [
        "### Shapley Values\n",
        "\n",
        "Next, we'll use SHAP to understand our model's predictions. SHAP values break down a prediction to show the contribution of each feature, helping us see which factors are most influential for a given data point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CJtLwWUe-KV"
      },
      "outputs": [],
      "source": [
        "from tabpfn_extensions import interpretability\n",
        "\n",
        "# Load example dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "n_samples_test, n_samples_train = 25, 50\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "\n",
        "# Initialize and train model\n",
        "clf = TabPFNClassifier()\n",
        "clf.fit(X_train[:n_samples_train], y_train[:n_samples_train])\n",
        "\n",
        "# Calculate SHAP values\n",
        "shap_values = interpretability.shap.get_shap_values(\n",
        "    estimator=clf,\n",
        "    test_x=X_test[:n_samples_test],\n",
        "    attribute_names=feature_names,\n",
        "    algorithm=\"permutation\",\n",
        ")\n",
        "\n",
        "# Create visualization\n",
        "fig = interpretability.shap.plot_shap(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt05caATl2Cc"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "This example demonstrates how to extract embeddings from TabPFN models and use them\n",
        "for classification and regression tasks.\n",
        "\n",
        "NOTE: This example requires the full TabPFN implementation (pip install tabpfn).\n",
        "It will not work with the TabPFN client (pip install tabpfn-client) because\n",
        "the embedding functionality is not available in the client version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj8XPK35l4z1"
      },
      "outputs": [],
      "source": [
        "if backend == \"client\":\n",
        "    console.print(\n",
        "        Panel(\n",
        "            \"[bold yellow]Text data can only be processed through the TabPFN server. Please restart the notebook and select the 'client' backend to run this section.[/bold yellow]\",\n",
        "            title=\"[bold yellow]Warning\",\n",
        "            border_style=\"yellow\",\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import seaborn as sns\n",
        "    from sklearn.datasets import load_breast_cancer, load_diabetes\n",
        "    from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.metrics import accuracy_score, r2_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from tabpfn_extensions import TabPFNClassifier, TabPFNRegressor\n",
        "    from tabpfn_extensions.embedding import TabPFNEmbedding\n",
        "\n",
        "    # Load and evaluate classification dataset\n",
        "    print(\"Loading classification dataset (breast cancer)...\")\n",
        "    df = load_breast_cancer(return_X_y=False)\n",
        "    X, y = df[\"data\"], df[\"target\"]\n",
        "    attribute_names = df[\"feature_names\"]\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.5,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Train and evaluate vanilla logistic regression\n",
        "    model = LogisticRegression(\n",
        "        max_iter=10000,\n",
        "        random_state=42,\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\n",
        "        f\"Baseline Logistic Regression Accuracy: {accuracy_score(y_test, model.predict(X_test)):.4f}\",\n",
        "    )\n",
        "\n",
        "    # Train and evaluate TabPFN embeddings (vanilla)\n",
        "    clf = TabPFNClassifier(n_estimators=1, random_state=42)\n",
        "    embedding_extractor = TabPFNEmbedding(tabpfn_clf=clf, n_fold=0)\n",
        "    train_embeddings = embedding_extractor.get_embeddings(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        data_source=\"train\",\n",
        "    )\n",
        "    test_embeddings = embedding_extractor.get_embeddings(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        data_source=\"test\",\n",
        "    )\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        max_iter=10000,\n",
        "        random_state=42,\n",
        "    )\n",
        "    model.fit(train_embeddings[0], y_train)\n",
        "    y_pred = model.predict(test_embeddings[0])\n",
        "    print(\n",
        "        f\"Logistic Regression with TabPFN (Vanilla) Accuracy: {accuracy_score(y_test, y_pred):.4f}\",\n",
        "    )\n",
        "\n",
        "    # Note: Using test_embeddings and y_test from your original script.\n",
        "    # The embeddings have shape (n_splits, n_samples, n_features), so we use the first split [0].\n",
        "    test_only_embeddings = test_embeddings[0]\n",
        "\n",
        "    # Apply t-SNE to reduce the TEST embeddings to 2 dimensions.\n",
        "    # The number of samples in the test set is len(y_test). Perplexity must be less than that.\n",
        "    # We'll set perplexity to 30, which is suitable for this dataset size.\n",
        "    print(\"\\nApplying t-SNE for visualization on the TEST SET only...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
        "    embeddings_2d = tsne.fit_transform(test_only_embeddings)\n",
        "\n",
        "    # Create a DataFrame for easy plotting\n",
        "    df_plot = pd.DataFrame(\n",
        "        {\n",
        "            \"t-SNE-1\": embeddings_2d[:, 0],\n",
        "            \"t-SNE-2\": embeddings_2d[:, 1],\n",
        "            \"label\": y_test,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Plot the 2D embeddings for the test set\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    sns.scatterplot(\n",
        "        data=df_plot,\n",
        "        x=\"t-SNE-1\",\n",
        "        y=\"t-SNE-2\",\n",
        "        hue=\"label\",  # Color points by their class label\n",
        "        palette=\"viridis\",\n",
        "        alpha=0.9,\n",
        "        s=80,\n",
        "    )\n",
        "\n",
        "    plt.title(\"t-SNE Visualization of TabPFN Embeddings (Test Set Only)\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend(title=\"Class\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkphS4vSFNvg"
      },
      "source": [
        "### Feature selection\n",
        "\n",
        "Feature selection is the process of selecting a subset of relevant features for use in model construction. It's useful for reducing model complexity, improving performance by removing noise, and decreasing training time. Here, we'll use Sequential Forward Selection (SFS), which starts with no features and iteratively adds the feature that most improves the model's performance.\n",
        "\n",
        "The goal is to see if we can create a simpler, faster model with fewer features without a significant drop in accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPJtNDLnFPPW"
      },
      "outputs": [],
      "source": [
        "from tabpfn_extensions import interpretability\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Initialize model\n",
        "clf = TabPFNClassifier(n_estimators=1)\n",
        "\n",
        "# Feature selection\n",
        "sfs = interpretability.feature_selection.feature_selection(\n",
        "    estimator=clf, X=X, y=y, n_features_to_select=4, feature_names=feature_names\n",
        ")\n",
        "\n",
        "# Print selected features\n",
        "selected_features = [\n",
        "    feature_names[i] for i in range(len(feature_names)) if sfs.get_support()[i]\n",
        "]\n",
        "print(\"\\nSelected features:\")\n",
        "for feature in selected_features:\n",
        "    print(f\"- {feature}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmPJW5NSK7ut"
      },
      "source": [
        "# Predictive Behavior of TabPFN <a name=\"behavior\"></a>\n",
        "\n",
        "In this section, we explore the behavior of predictions from TabPFN on various toy functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBeID4YeMQ9L"
      },
      "source": [
        "## Decision Boundary during Classification\n",
        "\n",
        "First, let us inspect the decision boundary for different classifiers.\n",
        "The decision bounds show how smoothly a model learns to transition between classes and how well it fits the data.\n",
        "\n",
        "In the first cell, we create the data and code to plot the decision boundary. The second cell then generates the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5a_ia0OLOxG"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "\n",
        "# Toy functions that generate the data\n",
        "def generate_circle(n_datapoints, radius, noise_factor):\n",
        "    angles = np.linspace(0, 2 * np.pi, n_datapoints).T\n",
        "    x = radius * np.cos(angles) + np.random.randn(n_datapoints) * noise_factor\n",
        "    y = radius * np.sin(angles) + np.random.randn(n_datapoints) * noise_factor\n",
        "\n",
        "    return np.stack([x, y]).T\n",
        "\n",
        "\n",
        "def generate_concentric_cirlces(radii, num_points_per_circle, noise_factor=1 / 15):\n",
        "    circles = []\n",
        "    for r, num_points in zip(radii, num_points_per_circle):\n",
        "        circles.append(generate_circle(num_points, r, noise_factor))\n",
        "\n",
        "    return np.vstack(circles)\n",
        "\n",
        "\n",
        "def generate_circle_data(num_points_per_circle, radii, noise_factor):\n",
        "    radii = np.array(radii)\n",
        "    circles_1 = generate_concentric_cirlces(radii, num_points_per_circle, noise_factor)\n",
        "    circles_1 = np.hstack([circles_1, np.zeros((sum(num_points_per_circle), 1))])\n",
        "\n",
        "    circles_2 = generate_concentric_cirlces(\n",
        "        radii + 0.3, num_points_per_circle, noise_factor\n",
        "    )\n",
        "    circles_2 = np.hstack([circles_2, np.ones((sum(num_points_per_circle), 1))])\n",
        "\n",
        "    circles = np.vstack([circles_1, circles_2])\n",
        "    X, y = circles[:, :2], circles[:, 2]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Generate the data\n",
        "X_train, y_train = generate_circle_data(\n",
        "    num_points_per_circle=[50, 100, 200], radii=[1, 2, 4], noise_factor=0.1\n",
        ")\n",
        "\n",
        "\n",
        "# Function for plotting\n",
        "def plot_decision_boundary(ax, model, model_name):\n",
        "    cmap = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "    ax.set_title(model_name)\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        model,\n",
        "        X_train[:, :2],\n",
        "        alpha=0.6,\n",
        "        ax=ax,\n",
        "        eps=0.2,\n",
        "        grid_resolution=50,\n",
        "        response_method=\"predict_proba\",\n",
        "        cmap=plt.cm.RdBu,\n",
        "    )\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train > 0, cmap=cmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzrSEZR5LYlf"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier().fit(X_train[:, :2], y_train)\n",
        "xgb = XGBClassifier().fit(X_train[:, :2], y_train)\n",
        "tabpfn = TabPFNClassifier().fit(X_train[:, :2], y_train)\n",
        "\n",
        "# Create a 2x2 subplot layout\n",
        "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
        "\n",
        "# Plot Train Points\n",
        "ax_points = axes[0, 0]\n",
        "ax_points.set_title(\"Train points\")\n",
        "ax_points.scatter(\n",
        "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        ")\n",
        "\n",
        "# Plot Random Forest\n",
        "ax_rf = axes[0, 1]\n",
        "plot_decision_boundary(ax_rf, rf, \"Random Forest\")\n",
        "\n",
        "# Plot XGBoost\n",
        "ax_xgb = axes[1, 0]\n",
        "plot_decision_boundary(ax_xgb, xgb, \"XGBoost\")\n",
        "\n",
        "# Plot TabPFN\n",
        "ax_tabpfn = axes[1, 1]\n",
        "plot_decision_boundary(ax_tabpfn, tabpfn, \"TabPFN\")\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tvOmmiOOT4o"
      },
      "source": [
        "## Sin Curve Fitting with Regression\n",
        "\n",
        "Next, we investigate the curve-fitting behavior of the different models on a sin curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_6wB1N2OrYu"
      },
      "outputs": [],
      "source": [
        "# Toy function to generate data\n",
        "def generate_sinx_plus_x(N):\n",
        "    x = np.linspace(0, 20, N)\n",
        "    y = np.sin(x) + x / 10\n",
        "\n",
        "    return x.reshape(-1, 1), y\n",
        "\n",
        "\n",
        "# Utility function for plotting\n",
        "def plot_predictions(ax, model, model_name):\n",
        "    X_test = np.linspace(0, 20, 200).reshape(\n",
        "        -1, 1\n",
        "    )  # Predict for 200 points  between 0 and 20\n",
        "    y_preds = model.predict(X_test)\n",
        "    ax.set_title(model_name)\n",
        "    ax.scatter(X_test, y_preds, label=\"Predictions\")\n",
        "    ax.scatter(X_train, y_train, label=\"Train points\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "# Generate data\n",
        "X_train, y_train = generate_sinx_plus_x(N=40)\n",
        "\n",
        "# Fit Models\n",
        "rf = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n",
        "xgb = XGBRegressor(random_state=42).fit(X_train, y_train)\n",
        "tabpfn = TabPFNRegressor()\n",
        "tabpfn.fit(X_train, y_train)\n",
        "\n",
        "# Create a 2x2 subplot layout\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Plot train points\n",
        "ax_points = axs[0, 0]\n",
        "ax_points.set_title(\"Train points\")\n",
        "ax_points.scatter(X_train, y_train, label=\"Train points\")\n",
        "ax_points.legend()\n",
        "\n",
        "# Plot predictions for Random Forest\n",
        "ax_rf = axs[0, 1]\n",
        "plot_predictions(ax_rf, rf, \"Random Forest\")\n",
        "\n",
        "# Plot predictions for XGBoost\n",
        "ax_xgb = axs[1, 0]\n",
        "plot_predictions(ax_xgb, xgb, \"XGBoost\")\n",
        "\n",
        "# Plot predictions for TabPFN\n",
        "ax_tabpfn = axs[1, 1]\n",
        "plot_predictions(ax_tabpfn, tabpfn, \"TabPFN\")\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYaJKT_Pw6T"
      },
      "source": [
        "## Uncertainty of TabPFN / Quantile Regresison\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "TabPFN is trained to predict the distribution of the target variable and not just a single point. This is also the case for regression.\n",
        "\n",
        "As a result, we natively obtain uncertainty for the predictions of TabPFN (without the need for a new model or repeating models for different quantiles).\n",
        "\n",
        "In the following, we plot the uncertainty of TabPFN for a toy function with noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5q1UBos4W1O"
      },
      "outputs": [],
      "source": [
        "### Code to generate the plot (feel free to ignore)\n",
        "def plot_regression_uncertainty(ax, x, y_line, y_noisy, x_test, color_multiplier=5):\n",
        "    all_quantiles = preds[\"quantiles\"]\n",
        "    y = np.array(all_quantiles)  # shape of y: (9, number of samples)\n",
        "\n",
        "    # Calculate the maximum and minimum values in y\n",
        "    y_max = np.max(y, axis=0)\n",
        "    y_min = np.min(y, axis=0)\n",
        "\n",
        "    # Calculate the widths of each quantile bin\n",
        "    quantile_bin_widths = np.diff(\n",
        "        y, axis=0\n",
        "    )  # shape of quantile_bin_widths: (8, number of samples)\n",
        "\n",
        "    # Normalize the bin widths for a given x\n",
        "    per_x_normalized_bin_widths = (quantile_bin_widths) / (y_max - y_min)\n",
        "\n",
        "    # Plotting\n",
        "    num_bins, num_data_points = (\n",
        "        per_x_normalized_bin_widths.shape[0],\n",
        "        per_x_normalized_bin_widths.shape[1],\n",
        "    )\n",
        "    rect_width = (\n",
        "        x_test[1] - x_test[0]\n",
        "    ).squeeze()  # assuming the x query points are equally spaced out\n",
        "\n",
        "    for i in range(num_data_points):\n",
        "        for j in range(num_bins):\n",
        "            quantile_bin_widths[j, i]\n",
        "            rect = plt.Rectangle(\n",
        "                xy=(x_test[i][0] - rect_width / 2, y[j, i]),\n",
        "                width=rect_width,\n",
        "                height=quantile_bin_widths[j, i],\n",
        "                facecolor=plt.cm.viridis(\n",
        "                    per_x_normalized_bin_widths[j, i] * color_multiplier\n",
        "                ),\n",
        "                edgecolor=\"none\",\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    # Set limits and labels\n",
        "    ax.plot(x, y_line, label=\"True line\", color=\"blue\")\n",
        "    ax.set_xlim(min(x_test) - 1, max(x_test) + 1)\n",
        "    ax.set_ylim(np.min(y_noisy) - 1, np.max(y_noisy) + 1)\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "\n",
        "    # ax.plot(x, y, label='True line', color='blue')\n",
        "    ax.scatter(x, y_noisy, label=\"Noisy data\", color=\"red\", s=10)\n",
        "\n",
        "    plt.title(\"TabPFN Regression Uncertainty\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate Data\n",
        "def generate_line_with_heteroscedastic_noise(\n",
        "    m=0.8, num_points=200, x_start=0, x_end=100, noise_factor=0.1\n",
        "):\n",
        "    x = np.linspace(x_start, x_end, num_points)\n",
        "    noise = np.random.randn(num_points)\n",
        "    y = m * x\n",
        "    y_noisy = y + (noise * noise_factor * x)\n",
        "\n",
        "    return x.reshape(-1, 1), y, y_noisy\n",
        "\n",
        "\n",
        "def generate_line_with_heteroscedastic_noise_with_gap(\n",
        "    m=0.8, num_points=200, x_start=0, x_end=100, noise_factor=0.1\n",
        "):\n",
        "    extra_points = num_points // 3\n",
        "    x, y, y_noisy = generate_line_with_heteroscedastic_noise(\n",
        "        m, num_points + extra_points, x_start, x_end, noise_factor\n",
        "    )\n",
        "    a = np.arange(num_points + extra_points)\n",
        "    idx = np.where((a < num_points / 3) | (a > num_points / 3 + extra_points))\n",
        "    return x[idx], y[idx], y_noisy[idx]\n",
        "\n",
        "\n",
        "x, y_line, y_noisy = generate_line_with_heteroscedastic_noise_with_gap(0.8)\n",
        "x_test = np.linspace(0, 100, 200).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpvqsL1f4W1O"
      },
      "outputs": [],
      "source": [
        "reg = TabPFNRegressor()\n",
        "reg.fit(x, y_noisy)\n",
        "preds = reg.predict(x_test, output_type=\"full\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwSCwFm-4W1O"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the original data\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(x, y_line, label=\"True line\", color=\"blue\")\n",
        "ax.scatter(x, y_noisy, label=\"Noisy data\", color=\"red\", s=10)\n",
        "ax.set_xlabel(\"X\")\n",
        "ax.set_ylabel(\"Y\")\n",
        "ax.set_xlim(min(x) - 1, max(x) + 1)\n",
        "ax.set_ylim(np.min(y_noisy) - 1, np.max(y_noisy) + 1)\n",
        "ax.set_title(\"Heteroscedastic Noise along a Sloping Line\")\n",
        "ax.legend()\n",
        "\n",
        "# plot the uncertainty estimates obtained from TabPFN\n",
        "ax = fig.add_subplot(122)\n",
        "plot_regression_uncertainty(ax, x, y_line, y_noisy, x_test, color_multiplier=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMcv-KvPYIh"
      },
      "source": [
        "# Time Series Forecasting <a name=\"timeseries\"></a>\n",
        "\n",
        "TabPFN can be used for time series prediction, following the work of Hoo et al. (Zero-Shot Time Series Forecasting with TabPFNv2; https://arxiv.org/abs/2501.02945v3).\n",
        "\n",
        "In this demo, we will use the time series dataset from [Chronos Dataset](https://huggingface.co/datasets/autogluon/chronos_datasets) on HuggingFace. Also, we'll keep it short and work with just 2 time series from the dataset.\n",
        "\n",
        "Feel free to explore other datasets by using the dataset names from Chronos Dataset on HuggingFace or even use time series data of your own problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3wiT00Uf0dF"
      },
      "source": [
        "## *Installation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0cUKia3fzrz"
      },
      "outputs": [],
      "source": [
        "!uv pip install tabpfn-time-series\n",
        "raise ValueError(\n",
        "    \"Make sure to restart the session before proceeding, otherwise errors will follow (Runtime > Restart Session)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNOAk5sPHS_3"
      },
      "source": [
        "## Load Time Series Data\n",
        "\n",
        "In this demo, we will use the time series dataset from [Chronos Dataset](https://huggingface.co/datasets/autogluon/chronos_datasets) on HuggingFace. Also, we'll keep it short and work with just 2 time series from the dataset.\n",
        "\n",
        "Feel free to explore other datasets by using the dataset names from Chronos Dataset on HuggingFace or even use time series data of your own problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgHudyQdHS_3"
      },
      "outputs": [],
      "source": [
        "dataset_metadata = {\n",
        "    \"monash_tourism_monthly\": {\"prediction_length\": 24},\n",
        "    \"m4_hourly\": {\"prediction_length\": 48},\n",
        "}\n",
        "\n",
        "dataset_choice = \"monash_tourism_monthly\"\n",
        "num_time_series_subset = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc1mHevjHS_4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tabpfn_time_series import TimeSeriesDataFrame\n",
        "from tabpfn_time_series.data_preparation import generate_test_X, to_gluonts_univariate\n",
        "\n",
        "prediction_length = dataset_metadata[dataset_choice][\"prediction_length\"]\n",
        "dataset = load_dataset(\"autogluon/chronos_datasets\", dataset_choice)\n",
        "\n",
        "tsdf = TimeSeriesDataFrame(to_gluonts_univariate(dataset[\"train\"]))\n",
        "tsdf = tsdf[\n",
        "    tsdf.index.get_level_values(\"item_id\").isin(tsdf.item_ids[:num_time_series_subset])\n",
        "]\n",
        "train_tsdf, test_tsdf_ground_truth = tsdf.train_test_split(\n",
        "    prediction_length=prediction_length\n",
        ")\n",
        "test_tsdf = generate_test_X(train_tsdf, prediction_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKH8WtXdHS_4"
      },
      "source": [
        "Let's take a look at the time series data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOm8_GLEHS_4"
      },
      "outputs": [],
      "source": [
        "from tabpfn_time_series.plot import plot_actual_ts\n",
        "\n",
        "plot_actual_ts(train_tsdf, test_tsdf_ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWmQq17tHS_4"
      },
      "source": [
        "## Adding Features\n",
        "\n",
        "In our paper, we propose adding `Running Index`, `Calendar Features`, and `Auto Seasonal Features` to the table.\n",
        "\n",
        "**Feel free to experiment with your own features!**\n",
        "\n",
        "To do that, simply define your own feature functions and pass them to the `FeatureTransformer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKWF7CrmHS_5"
      },
      "outputs": [],
      "source": [
        "from tabpfn_time_series import FeatureTransformer\n",
        "from tabpfn_time_series.features import (\n",
        "    AutoSeasonalFeature,\n",
        "    CalendarFeature,\n",
        "    RunningIndexFeature,\n",
        ")\n",
        "\n",
        "selected_features = [\n",
        "    RunningIndexFeature(),\n",
        "    CalendarFeature(),\n",
        "    AutoSeasonalFeature(),\n",
        "]\n",
        "\n",
        "feature_transformer = FeatureTransformer(selected_features)\n",
        "\n",
        "train_tsdf, test_tsdf = feature_transformer.transform(train_tsdf, test_tsdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9WZ46ywJylH"
      },
      "source": [
        "Let's take a look at the tables (train and test) before we proceed to do predictions.\n",
        "\n",
        "✅ Realize that we have added some features into the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnKwZbngJx0X"
      },
      "outputs": [],
      "source": [
        "train_tsdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKaNq3RAKMH9"
      },
      "outputs": [],
      "source": [
        "test_tsdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqNGS6a0HS_5"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAh9dvY5HS_5"
      },
      "source": [
        "Now, let's perform prediction.\n",
        "\n",
        "We provide two options, `TabPFNMode.LOCAL` and `TabPFNMode.CLIENT`, as the backend for TabPFN.\n",
        "\n",
        "- `TabPFNMode.LOCAL` uses your local machine to run TabPFN.\n",
        "- `TabPFNMode.CLIENT` uses TabPFN's inference service provided by [tabpfn-client](https://github.com/automl/tabpfn-client)\n",
        "\n",
        "For this demo, we'll use `TabPFNMode.CLIENT` to perform prediction. If you have not use the client before, you'll be prompted to create an account.\n",
        "\n",
        "Note: if your machine doesn't have a GPU, using `TabPFNMode.CLIENT` is recommended -- must faster 😉.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_gi5gDFHS_5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tabpfn_time_series import TabPFNMode, TabPFNTimeSeriesPredictor\n",
        "\n",
        "predictor = TabPFNTimeSeriesPredictor(\n",
        "    tabpfn_mode=TabPFNMode.LOCAL,  # adapt this to TabPFNMode.CLIENT if using API\n",
        ")\n",
        "\n",
        "pred = predictor.predict(train_tsdf, test_tsdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHiW8LeHS_5"
      },
      "source": [
        "## Visualize the Results\n",
        "\n",
        "Let's visualize the forecasting results.\n",
        "\n",
        "Also, note that we provide both **point prediction** and **quantile prediction**, how amazing! 😄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCxt23reHS_5"
      },
      "outputs": [],
      "source": [
        "from tabpfn_time_series.plot import plot_pred_and_actual_ts\n",
        "\n",
        "plot_pred_and_actual_ts(\n",
        "    train=train_tsdf,\n",
        "    test=test_tsdf_ground_truth,\n",
        "    pred=pred,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EJU0ILwHS_5"
      },
      "outputs": [],
      "source": [
        "pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using TabPFN as a Base Model for Causal Inference <a name=\"causal\"></a>\n",
        "\n",
        "Often decisions hinge on **what changes because of an action**, not just what correlates with it. Causal effect estimation turns data into these counterfactual answers—e.g., _If we treat someone like xxx, how much does the expected outcome change?_\n",
        "\n",
        "Below we show how to use TabPFN to estimation the effect of these causal intervations.\n",
        "\n",
        "*In depth explanation:*\n",
        "\n",
        "To estimate it most methods build two ingredients: an **outcome model** (predict yyy under T\\=1T{=}1T\\=1 and T\\=0T{=}0T\\=0) and often a **propensity model** (estimate Pr⁡(T\\=1∣X)\\\\Pr(T{=}1\\\\mid X)Pr(T\\=1∣X)) to reduce selection bias. You can treat this as AutoML, or take a strong default: **TabPFN**, a pretrained transformer for tabular data that frequently works **out of the box** as the base learner inside CATE pipelines—cutting tuning time while staying competitive.\n",
        "\n",
        "\n",
        "Many causal inference methods require selection of base models to fit either propensity or outcome models. While recent studies have framed this as an AutoML model selection problem ([Vandershueren et al, 2025](https://openreview.net/forum?id=QbOoz74GNO)), other studies ([Zhang et al, 2025](https://arxiv.org/pdf/2505.20003)) have shown TabPFN to be a strong choice \"out of the box\" in various CATE estimation methods. In parallel, there is a growing body of research pre-training PFNs specifically for the conditional average treatment effect (CATE) estimation task ([Robertson et al.](https://arxiv.org/pdf/2506.06039)), defined as:\n",
        "\n",
        "$$CATE : \\tau = E[y | do(1), x] - E[y | do(0), x]$$\n",
        "\n",
        "\n",
        "**NOTE:** Current CATE estimators only work when *unconfoundedness* $(Y_0, Y_1) \\perp \\kern-1.8ex \\perp T$ $|$ $X$ is satisfied. We are working on pre-training a foundation model called \"Do-PFN\" ([Robertson et al.](https://arxiv.org/pdf/2506.06039)) that is designed to offer strong out-of-the box performance in **all** identifiable scenarios (including front-door and instrumental variable settings)."
      ],
      "metadata": {
        "id": "0bBYgouXsh2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Install the required packages\n",
        "\n",
        "Note: you will need to restart the notebook (Runtime -> Restart Session)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yn_gs71ksBfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNU2S_pwrSq1"
      },
      "outputs": [],
      "source": [
        "!pip install tabpfn\n",
        "!pip install econml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Generate a synthetic CATE dataset\n",
        "\n",
        "The dataset $D = (T, X, Y, \\tau)$ below represents an observed confounder scenario $G = \\{X \\rightarrow Y, T \\rightarrow Y, X \\rightarrow T\\}$ and thus satisfies the unconfoundedness assumption: $(Y_0, Y_1) \\perp \\kern-1.8ex \\perp T$ $|$ $X$. We divide the dataset into train and test splits $(D_{train}, D_{test})$."
      ],
      "metadata": {
        "id": "4wlBhyRpy1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "edges = [[\"X\", \"Y\"], [\"T\", \"Y\"], [\"X\", \"T\"]]\n",
        "graph = nx.DiGraph(edges)\n",
        "nx.draw(graph, with_labels=True, node_size=2000)"
      ],
      "metadata": {
        "id": "pFK2RGwpzIWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples, num_features, train_test_split = 500, 5, 75\n",
        "\n",
        "noise_scale = 0.01\n",
        "exogenous_scale = 0.1\n",
        "heterogeneity_scale = 2\n",
        "\n",
        "X = np.random.normal(loc=0, scale=exogenous_scale, size=(num_samples, num_features)).astype(np.float32)\n",
        "prop_eps = np.random.normal(loc=0, scale=noise_scale, size=(num_samples)).astype(np.float32)\n",
        "Y_eps = np.random.normal(loc=0, scale=noise_scale, size=(num_samples)).astype(np.float32)\n",
        "\n",
        "w_X_Y = np.random.uniform(size=num_features)\n",
        "w_X_T = np.random.uniform(size=num_features)\n",
        "w_X_T_effect = np.random.uniform(size=num_features)\n",
        "\n",
        "T_0 = np.zeros(shape=num_samples).astype(np.float32)\n",
        "T_1 = np.ones(shape=num_samples).astype(np.float32)\n",
        "\n",
        "base_X_Y = np.sum(w_X_Y * X, axis=1)\n",
        "base_X_T = np.sum(w_X_T * X, axis=1)\n",
        "heterogeneity_term = np.sum(w_X_T_effect * X, axis=1)\n",
        "\n",
        "propensity = np.sin(base_X_T) + prop_eps\n",
        "T = (propensity > propensity.mean()).astype(np.float32)\n",
        "\n",
        "Y_0 = np.cos(base_X_Y) + np.sin(0.3 * T_0) + Y_eps\n",
        "Y_1 = np.cos(base_X_Y) + np.sin(0.3 * T_1 + heterogeneity_scale * heterogeneity_term) + Y_eps\n",
        "\n",
        "Y = Y_0 * (1 - T) + Y_1 * T\n",
        "tau = Y_1 - Y_0"
      ],
      "metadata": {
        "id": "gadFg7JusQ6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame({**{f\"X{i}\": X[:, i] for i in range(num_features)}, \"T\": T, \"Y\": Y, \"Y_0\": Y_0, \"Y_1\": Y_1, \"tau\": tau})\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(6, 3))\n",
        "sns.scatterplot(data=df, x=\"X0\", y=\"tau\", hue=\"T\", ax=axes[0], edgecolor=\"black\", palette=\"Set1\")\n",
        "sns.scatterplot(data=df, x=\"X1\", y=\"tau\", hue=\"T\", ax=axes[1], edgecolor=\"black\", palette=\"Set1\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "VnBaoFhLBiKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perm = np.random.permutation(num_samples)\n",
        "train_idx, test_idx = perm[:train_test_split], perm[train_test_split:]\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
        "T_train, T_test = T[train_idx], T[test_idx]\n",
        "tau_test = tau[test_idx]"
      ],
      "metadata": {
        "id": "3KJJwIwSySzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run CATE estimators with TabPFN as a base model\n",
        "\n",
        "We fit three CATE estimators to our synthetic datasets with different choices of base models\n",
        "\n",
        "\n",
        "\n",
        "*   `s_tabpfn`: TabPFN applied as an `SLearner`\n",
        "*   `cfdml_tabpfn`: TabPFN applied as a propensity and outcome in `CausalForestDML`\n",
        "*   `cfdml_default`: `CausalForestDML` with default propensity and outcome models\n",
        "\n"
      ],
      "metadata": {
        "id": "STYhNc224Zmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from econml.metalearners import SLearner\n",
        "from econml.dml import CausalForestDML\n",
        "from tabpfn import TabPFNClassifier, TabPFNRegressor"
      ],
      "metadata": {
        "id": "-3id6xmIrajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_tabpfn = SLearner(overall_model=TabPFNRegressor())\n",
        "\n",
        "s_tabpfn.fit(Y=Y_train, X=X_train, T=T_train)\n",
        "s_tabpfn_cate = s_tabpfn.effect(X=X_test)"
      ],
      "metadata": {
        "id": "xuZ1kxQP1ZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfdml_tabpfn = CausalForestDML(model_y=TabPFNRegressor(), model_t=TabPFNClassifier(), discrete_treatment=True)\n",
        "\n",
        "cfdml_tabpfn.fit(Y=Y_train, X=X_train, T=T_train)\n",
        "cfdml_tabpfn_cate = cfdml_tabpfn.effect(X=X_test)"
      ],
      "metadata": {
        "id": "1eUVnX-r1waQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfdml_default = CausalForestDML(discrete_treatment=True)\n",
        "\n",
        "cfdml_default.fit(Y=Y_train, X=X_train, T=T_train)\n",
        "cfdml_default_cate = cfdml_default.effect(X=X_test)"
      ],
      "metadata": {
        "id": "R_QEm9P12tlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate performance in CATE estimation\n",
        "\n",
        "We assess how close our predicted individual effects $\\hat{\\tau}(x)$ are to the true effects $\\tau(x)$ using **PEHE** (Precision in Estimation of Heterogeneous Effects).\n",
        "PEHE is defined as the root-mean-squared-error (RMSE) between predicted and ground truth CATE values.\n",
        "\n",
        "$$\\text{PEHE} = \\sqrt{\\frac{1}{n} \\sum [\\widehat{\\tau}(x_i) - \\tau(x_i) ]^2}$$\n"
      ],
      "metadata": {
        "id": "nbsS1tvf7GS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "s_tabpfn_pehe = root_mean_squared_error(tau_test, s_tabpfn_cate)\n",
        "cfdml_tabpfn_pehe = root_mean_squared_error(tau_test, cfdml_tabpfn_cate)\n",
        "cfdml_default_pehe = root_mean_squared_error(tau_test, cfdml_default_cate)\n",
        "\n",
        "print(f\"s_tabpfn PEHE: {s_tabpfn_pehe}\")\n",
        "print(f\"cfdml_tabpfn PEHE: {cfdml_tabpfn_pehe}\")\n",
        "print(f\"cfdml_default PEHE: {cfdml_default_pehe}\")"
      ],
      "metadata": {
        "id": "LQYrBYLB7F2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame({\"T\": T_test, \"Y\": Y_test, \"tau\": tau_test, **{f\"X{i}\": X_test[:, i] for i in range(num_features)}})\n",
        "df_test[\"s_tabpfn_pred\"], df_test[\"cfdml_tabpfn_pred\"], df_test[\"cfdml_default_pred\"] = s_tabpfn_cate, cfdml_tabpfn_cate, cfdml_default_cate\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, figsize=(9, 3))\n",
        "sns.scatterplot(data=df_test, x=\"tau\", y=\"s_tabpfn_pred\", hue=\"T\", ax=axes[0], edgecolor=\"black\", palette=\"Set1\")\n",
        "sns.scatterplot(data=df_test, x=\"tau\", y=\"cfdml_tabpfn_pred\", hue=\"T\", ax=axes[1], edgecolor=\"black\", palette=\"Set1\")\n",
        "sns.scatterplot(data=df_test, x=\"tau\", y=\"cfdml_default_pred\", hue=\"T\", ax=axes[2], edgecolor=\"black\", palette=\"Set1\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "-Jlg79mU_Ehg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlKBzRphHS_6"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Let us know what you think!\n",
        "\n",
        "Write us at Discord: https://discord.gg/qK7AaXPN or just simply create an issue on [GitHub](https://github.com/PriorLabs/tabpfn).\n",
        "\n",
        "Thank you for trying out our method! 🎉\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tisgLOuqzrfo",
        "o03aOVAw0Etg",
        "iro4vB3w4rgE",
        "c8iQ9KEd1h9R",
        "ZVs_HfxbVCs3",
        "ipG2Cpb7Jrc7",
        "3uGfjIdjsCg-",
        "mTnMbG9vXHkL",
        "6RYILEoPXaMZ",
        "PkphS4vSFNvg",
        "ZBeID4YeMQ9L",
        "8tvOmmiOOT4o",
        "XcYaJKT_Pw6T"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}